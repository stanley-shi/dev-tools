<?xml version="1.0" encoding="UTF-8" standalone="no"?><clusterConfig>
     <!-- Cluster Name.  This will be used on command lines, so avoid spaces and special characters. -->
     <clusterName>phd-2_1_0_0</clusterName>


     <gphdStackVer>PHD-2.1.0.0</gphdStackVer>

     <!-- Comma separated list.  Choices are: hdfs,yarn,zookeeper,hbase,hive,hawq,pig,mahout,gfxd -->
     <!-- services not meant to be installed can be deleted from the following list. -->
     <services>hdfs,yarn,zookeeper,hbase,hive,pig,mahout</services>

    <!-- Secure cluster. Set this parameter to true in order to secure cluster -->
    <securityEnabled>true</securityEnabled>

    <!-- Hostname of the machine which will be used as the client -->
     <!-- ICM will install Pig, Hive, HBase and Mahout libraries on this machine. -->
     <client>centos65-2.localdomain</client>

     <!-- Comma separated list of fully qualified hostnames should be 
          associated with each service role -->
     <!-- services (and corresponding roles) not meant to be installed can 
          be deleted from the following list -->
     <hostRoleMapping>
         <hdfs>
            <!-- HDFS, NameNode role (mandatory) -->
             <namenode>centos65-2.localdomain</namenode>

            <!-- HDFS, DataNode role (mandatory) ** list of one or more, comma separated, names must be resolvable ** -->
             <datanode>centos65-4.localdomain,centos65-5.localdomain</datanode>

            <!-- HDFS, Secondary NameNode role (optional) - ** comment out XML tag if not required ** -->
            <!--<secondarynamenode>centos65-1.localdomain</secondarynamenode>-->

             <!-- HDFS, Standby NameNode that acts as a failover when HA is enabled -->
             <standbynamenode>centos65-3.localdomain</standbynamenode>

             <!--specify quorum journal nodes used as common storage by both active and stand-by namenode.
              list of one or more comma separated  -->
             <journalnode>centos65-4.localdomain,centos65-5.localdomain,centos65-3.localdomain</journalnode>
         </hdfs>

         <yarn>
            <!-- YARN, ResourceManager role (mandatory) -->
             <yarn-resourcemanager>centos65-2.localdomain</yarn-resourcemanager>

            <!-- YARN, NodeManager role (mandatory) ** list of one or more, comma separated, generally takes on the DataNode element  ** -->
             <yarn-nodemanager>centos65-4.localdomain,centos65-5.localdomain</yarn-nodemanager>

            <!-- Generally runs on the ResourceManager (mandatory) -->
             <mapreduce-historyserver>centos65-3.localdomain</mapreduce-historyserver>
         </yarn>

         <zookeeper>
            <!-- ZOOKEEPER, zookeeper-server role (required if using HBase) ** list an *ODD NUMBER*, comma separated ** -->
            <!-- Options to run zookeepers: Master nodes like Secondary NameNode, ResourceManager, HAWQ Standby node,etc EXCEPT NameNode-->
             <zookeeper-server>centos65-3.localdomain</zookeeper-server>
         </zookeeper>

         <hbase>
             <!-- HBASE, master role -->
             <hbase-master>centos65-2.localdomain</hbase-master>

             <!-- HBASE, region server role ** list of one or more, comma separated ** -->
             <!-- HBase region servers are generally deployed on the DataNode hosts -->
             <hbase-regionserver>centos65-3.localdomain,centos65-4.localdomain,centos65-5.localdomain</hbase-regionserver>
         </hbase>

         <hive>
             <!-- HIVE, server role -->
             <!-- Hive thrift server would be running on this machine. -->
             <hive-server>centos65-4.localdomain</hive-server>

             <!-- HIVE, metastore role. This host will run an instance of PostgreSQL. -->
             <!-- Postgres database is installed on this node to maintain Hive metadata. -->
             <hive-metastore>centos65-5.localdomain</hive-metastore>
         </hive>

         <hawq>
             <!-- HAWQ, hawq master role (mandatory).  This should be a different host than the NameNode host. -->
             <hawq-master>centos65-1.localdomain</hawq-master>

             <!-- HAWQ, hawq standbymaster role (optional) - ** comment out XML tag if not required **-->
             <hawq-standbymaster>centos65-1.localdomain</hawq-standbymaster>

             <!-- HAWQ, hawq segments (mandatory) ** list of one or more, comma separated ** -->
             <!-- In many configurations, this will be the same list as the DataNode role. -->
             <hawq-segment>centos65-1.localdomain</hawq-segment>
         </hawq>

         <gfxd>
             <gfxd-locator>centos65-1.localdomain</gfxd-locator>
             <gfxd-server>centos65-1.localdomain</gfxd-server>
         </gfxd>

     <pxf>
        <!-- PXF, pxf services (mandatory) that is a webapp container ** list of one or more, comma separated ** -->
        <!-- Always collocate with all datanodes and the namenode -->
        <pxf-service>centos65-1.localdomain</pxf-service>
    </pxf>
</hostRoleMapping>

     <servicesConfigGlobals>
         <!-- List of one or more, comma separated, disk mount points on slave nodes (datanode/nodemanager/regionservers) -->
         <datanode.disk.mount.points>/data/1,/data/2,/data/3</datanode.disk.mount.points>

        <!-- List of one or more, comma separated, disk mount points on NameNode.
             More than one can be configured for NameNode local data redundancy. -->
        <namenode.disk.mount.points>/data/nn</namenode.disk.mount.points>

        <!-- List of one or more, comma separated disk mount points on secondary-namenode.
             More than one can be configured for secondary-namenode local data redundancy. -->

        <!-- <secondary.namenode.disk.mount.points>/data/secondary_nn</secondary.namenode.disk.mount.points> -->

        <!-- Max. memory on nodemanager collectively available to all the task containers including application master. --> 
         <yarn.nodemanager.resource.memory-mb>2048</yarn.nodemanager.resource.memory-mb>

        <!-- Min memory required for the task or application manager container   -->
         <yarn.scheduler.minimum-allocation-mb>256</yarn.scheduler.minimum-allocation-mb>

        <!-- THIS IS THE "NameNode port". -->
         <dfs.port>8020</dfs.port>

        <!-- This syntax correlates to a bash array data structure. There are no commas seperating the values. It's just whitespace.
             If you enter multiple entries with the exact same value; each entry will result in a HAWQ segment placed on a host.
             If you want two segments per hosts make two entries with the same values; which may or may not be placed on the same disk within the host.
             WHICH MAY OR MAY NOT BE ON THE SAME DISK.  BY CONVENTION, THE LAST PART OF THIS STRING IS
             In the example below, two segments will be created on every HAWQ segment host -->
         <hawq.segment.directory>(/data1/primary /data1/primary)</hawq.segment.directory>

        <!-- THIS IS A SINGLE VALUE, AND IDEALLY THE DIRECTORY SPECIFIED CORRELATES WITH A RAID VOLUME, AS THE
             MASTER DATA DIRECTORY CONTAINS IMPORTANT METADATA. -->
         <hawq.master.directory>/data1/master</hawq.master.directory>

        <!-- IDEALLY, THIS DIRECTORY IS ON ITS OWN DEVICE, TO MINIMIZE I/O CONTENTION BETWEEN ZOOKEEPER
             AND OTHER PROCESSES. -->
         <zookeeper.data.dir>/var/lib/zookeeper</zookeeper.data.dir>

         <!-- Zookeeper Client Port -->
         <zookeeper.client.port>2181</zookeeper.client.port>

         <!-- JAVA_HOME defaults to this. Update if you have java installed elsewhere -->
         <cluster_java_home>/usr/java/default</cluster_java_home>

         <!--maximum amount of HEAP to use. Default is 1024-->
         <dfs.namenode.heapsize.mb>1024</dfs.namenode.heapsize.mb>
         <dfs.datanode.heapsize.mb>1024</dfs.datanode.heapsize.mb>
         <yarn.resourcemanager.heapsize.mb>1024</yarn.resourcemanager.heapsize.mb>
         <yarn.nodemanager.heapsize.mb>1024</yarn.nodemanager.heapsize.mb>
         <hbase.heapsize.mb>1024</hbase.heapsize.mb>

         <dfs.datanode.failed.volumes.tolerated>0</dfs.datanode.failed.volumes.tolerated>

         <!-- Choose a logical name for HA nameservice, for example "test". The name you choose will be used both for configuration
                 and as the authority component of absolute HDFS paths in the cluster -->
         <nameservices>phd-ns1</nameservices>

         <!-- Choose ids for the two namenodes being used. These ids will be used in the configuration of properties related to these namenodes -->
         <namenode1id>nn1</namenode1id>
         <namenode2id>nn2</namenode2id>

         <!--specify the path on quorum journal nodes where the shared data is written -->
         <journalpath>/data/qjournal/namenode</journalpath>
         <!--specify the port where quorum journal nodes should be run-->
         <journalport>8485</journalport>

         <!-- Security configurations -->
         <!-- provide security realm. e.g. EXAMPLE.COM -->
         <security.realm>LOCALDOMAIN</security.realm>
         <!-- provide the path of kdc conf file -->
         <security.kdc.conf.location>/etc/krb5.conf</security.kdc.conf.location>

     </servicesConfigGlobals>

 </clusterConfig>
